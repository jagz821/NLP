{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning \n",
    "Text pre-processing is one of the major tasks in NLP. To get useful insights from the data we may want to remove punctuations, numbers, special characters etc. and to make this task simpler we use REGEX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REGEX also known as Regular Expression is used to search for a pattern in the data. REGEX contain two components.\n",
    "<b>1)</b> Literals - they have no syntax meaning like (a,b,1,2)<br>\n",
    "<b>2)</b> Metacharacters - They are made up of symbol and alphabets and  have good syntax meaning (?,*). Ex: . - for character, d- digits\n",
    "\n",
    "<b> To use the metacharacter as a literal we will use escaping \"\\\\\"</b>\n",
    "\n",
    "----\n",
    "The most common usage of REGEX are:<br>\n",
    "\n",
    "<b>1)</b> Search - Search inside the string and stop when the pattern is found.<br>\n",
    "<b>2)</b> Match - Search the pattern at the beginning of the string.<br>\n",
    "<b>3)</b> findall - Find all the occurence.<br>\n",
    "<b>4)</b> sub - Replace part of the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 31), match='India won the world cup in 2012'>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Match (string of any length or character)\n",
    "text = \"India won the world cup in 2012\" \n",
    "result = re.match(r\".*\",text)# regex expression start with alphabet r and search for any character except newline\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 24), match='India won the world cup '>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Match string of any length or character\n",
    "text = \"India won the world cup \\n in 2012\" \n",
    "result = re.match(r\".*\",text)# regex expression start with alphabet r and search for any character except newline\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found match at 1 and the string is a\n"
     ]
    }
   ],
   "source": [
    "# Search\n",
    "pattern = 'a'\n",
    "search_ = re.search(pattern,'Machine Learning')\n",
    "print(\"Found match at\",search_.start(),\"and the string is\",search_.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'a']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#findall\n",
    "pat = 'a'\n",
    "search_ = re.findall(pattern,'aeroplane')\n",
    "search_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*eropl*ne'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sub \n",
    "pat = 'a'\n",
    "re.sub(pat,'*','aeroplane')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metacharacters \n",
    "* . for any character\n",
    "* \\d for digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character class\n",
    "s1 = 'defense'\n",
    "s2 = 'defence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 7), match='defense'>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(pattern = r'defen[sc]e',string= s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 7), match='defence'>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(pattern = r'defen[sc]e',string= s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence character class can be used as OR statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Regex\n",
    "* <b>Alphabets [a-z] and [A-Z] \n",
    "* Numbers [0-9]\n",
    "* ^ Negation [^0-9] -- Search everything except Numbers\n",
    "* [0-9a-zA-z] -- Alphanumeric  \n",
    "* [^0-9a-zA-z] -- only symbols\n",
    "* \\s -- for white space [\\n, \\t, \\v, \\h]\n",
    "* <i> to use s as s- alphabet we use \"s\"</i>\n",
    "* \\w = [a-zA-Z0-9]\n",
    "* \\W = ^[a-zA-Z0-9]\n",
    "* \\d = Digits[0-9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching Alphabets\n",
    "text = \"Covid-19 is increasing at an alarming rate\"\n",
    "result = re.match(r\"[a-zA-Z]+\",text)\n",
    "# this regular expression has searched for alphabet and stopped when a number found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 5), match='Covid'>\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching Numbers & speccial characters\n",
    "text = \"Covid-19 is increasing at an alarming rate\"\n",
    "result = re.search(r\"[^a-zA-Z]+\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(5, 9), match='-19 '>\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Alphabets ['J', 'a', 'g', 'A', 'B', 'C', 'D', 'h', 'f', 'h', 'i', 'o', 'h', 'i', 'f', 'h', 'i']\n"
     ]
    }
   ],
   "source": [
    "# flag ignore to ignore case\n",
    "print(\"All Alphabets\",re.findall(pattern= r'[a-z]',string = \"JagABCDhfhio3hifhi821\",flags=re.I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n']\n"
     ]
    }
   ],
   "source": [
    "# space, tab, newline\n",
    "pat = '\\s'\n",
    "string = 'heiiurfihfi....7@#$$\\nhduiw523'\n",
    "print(re.findall(pat,string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Passwordhjj \n",
      "Don't use space\n"
     ]
    }
   ],
   "source": [
    "Password = input(\"Enter Password\")\n",
    "obj = re.search(r'\\s',Password)\n",
    "if obj == None:\n",
    "    print(\"Correct Password\")\n",
    "else:\n",
    "    print(\"Don't use space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifiers\n",
    "#### - No occurence(?)\n",
    "#### - Exactly one(*)\n",
    "#### - Atleast one(+)\n",
    "#### - Exactly n times(n)\n",
    "#### - min m times max n times(m,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 10), match='7856666666'>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find numbers beginning with 6,7,8,9 or 9 followed by 9 digits\n",
    "pat = \"[6-9]\\d{9}\"\n",
    "result = re.match(pat,\"785666666666\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"9xm\"\n",
    "s2 = \"MTV\"\n",
    "pat = r'[0-9]?'\n",
    "searchObj = re.search( r'[0-9]?', s1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='9'>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchObj = re.search( r'[0-9]?', s2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 0), match=''>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['987685']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Anchors for start and end\n",
    "pat = \"^[6-9]\\d{5}$\" # start with either 6,7,8,9 and at the end i need 5 more nubers\n",
    "re.findall(pat,string = \"987685\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found\n"
     ]
    }
   ],
   "source": [
    "text = \"1998 was the year when the film titanic was released\"\n",
    "if re.search(r\"1998$\", text):\n",
    "    print(\"Match found\")\n",
    "else:\n",
    "    print(\"Match not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning \n",
    "Machines can’t understand the unstructured raw data, and we can’t fit our model on the raw text. We have to clean the text and convert it from the unstructured format to a structured format to get any meaning from the data. Text cleaning is a crucial part of any NLP project.\n",
    "\n",
    "-----------\n",
    "<b>Now that we are dealing with text data we need to understand some basic terms:</b>\n",
    "* Words:- Collection of alphabets without any symbols or numbers.\n",
    "* Term:- Important word in any language except stopwords(General words).\n",
    "* Document:- The atomic part of the text data. Ex: Tweet in Twitter data.\n",
    "* Corpus:- The most important and required document for the analysis. A corpus is a systematic computerized collection of authentic language that is used for linguistic analysis as well as corpus analysis. \n",
    "----------\n",
    "<b>Steps in Data Cleaning:</b>\n",
    "* Remove HTML characters from corpus.- <i>Using Regex</i>\n",
    "* Convert the text to lower case ( Case Standardization).\n",
    "* Remove Punctuations.\n",
    "* Remove Stopwords.\n",
    "* Tokenization.\n",
    "* Stemming vs Lemmatization.\n",
    "* Get Parts of Speech.\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet # is for word meanings\n",
    "import nltk.classify.util\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence =  \"\"\"Thank you all so very much. Thank you to the Academy. \n",
    "    Thank you to all of you in this room. I have to congratulate \n",
    "               the other incredible nominees this year. The Revenant was \n",
    "               the product of the tireless efforts of an unbelievable cast\n",
    "               and crew. First off, to my brother in this endeavor, Mr. Tom \n",
    "               Hardy. Tom, your talent on screen can only be surpassed by \n",
    "               your friendship off screen … thank you for creating a t\n",
    "               ranscendent cinematic experience. Thank you to everybody at \n",
    "               Fox and New Regency … my entire team. I have to thank \n",
    "               everyone from the very onset of my career … To my parents; \n",
    "               none of this would be possible without you. And to my \n",
    "               friends, I love you dearly; you know who you are. And lastly,\n",
    "               I just want to say this: Making The Revenant was about\n",
    "               man's relationship to the natural world. A world that we\n",
    "               collectively felt in 2015 as the hottest year in recorded\n",
    "               history. Our production needed to move to the southern\n",
    "               tip of this planet just to be able to find snow. Climate\n",
    "               change is real, it is happening right now. It is the most\n",
    "               urgent threat facing our entire species, and we need to work\n",
    "               collectively together and stop procrastinating. We need to\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. For our children’s children, and \n",
    "               for those people out there whose voices have been drowned\n",
    "               out by the politics of greed. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there aren't any html characters and hence we will convert it into lower case\n",
    "sentence = sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Punctuations\n",
    "sentence_P = re.sub(r\"[^a-zA-Z ]\",\"\",sentence)\n",
    "#substitute nothing if encounter anything except a-zA-Z (Alphabets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('English'))\n",
    "#these are the stopwords in english language and we will remove all of them as they have no meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Removing stopwords\n",
    "sentence_split = sentence_P.split(\" \")\n",
    "new_words = []\n",
    "for i in sentence_split:\n",
    "    if i not in stopwords.words(\"English\") and i != \"\":\n",
    "        new_words.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thank', 'much', 'thank', 'academy', 'thank', 'room', 'congratulate', 'incredible', 'nominees', 'year', 'revenant', 'product', 'tireless', 'efforts', 'unbelievable', 'cast', 'crew', 'first', 'brother', 'endeavor', 'mr', 'tom', 'hardy', 'tom', 'talent', 'screen', 'surpassed', 'friendship', 'screen', 'thank', 'creating', 'ranscendent', 'cinematic', 'experience', 'thank', 'everybody', 'fox', 'new', 'regency', 'entire', 'team', 'thank', 'everyone', 'onset', 'career', 'parents', 'none', 'would', 'possible', 'without', 'friends', 'love', 'dearly', 'know', 'lastly', 'want', 'say', 'making', 'revenant', 'mans', 'relationship', 'natural', 'world', 'world', 'collectively', 'felt', 'hottest', 'year', 'recorded', 'history', 'production', 'needed', 'move', 'southern', 'tip', 'planet', 'able', 'find', 'snow', 'climate', 'change', 'real', 'happening', 'right', 'urgent', 'threat', 'facing', 'entire', 'species', 'need', 'work', 'collectively', 'together', 'stop', 'procrastinating', 'need', 'support', 'leaders', 'around', 'world', 'speak', 'big', 'polluters', 'speak', 'humanity', 'indigenous', 'people', 'world', 'billions', 'billions', 'underprivileged', 'people', 'would', 'affected', 'childrens', 'children', 'people', 'whose', 'voices', 'drowned', 'politics', 'greed', 'thank', 'amazing', 'award', 'tonight', 'let', 'us', 'take', 'planet', 'granted', 'take', 'tonight', 'granted', 'thank', 'much']\n"
     ]
    }
   ],
   "source": [
    "print(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Correction\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"Englash\"\n",
    "tb = TextBlob(word)\n",
    "r = tb.correct().raw\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank',\n",
       " 'much',\n",
       " 'thank',\n",
       " 'academy',\n",
       " 'thank',\n",
       " 'room',\n",
       " 'congratulate',\n",
       " 'incredible',\n",
       " 'nominee',\n",
       " 'year',\n",
       " 'covenant',\n",
       " 'product',\n",
       " 'tireless',\n",
       " 'efforts',\n",
       " 'unbelievable',\n",
       " 'cast',\n",
       " 'crew',\n",
       " 'first',\n",
       " 'brother',\n",
       " 'endeavor',\n",
       " 'mr',\n",
       " 'tom',\n",
       " 'hardy',\n",
       " 'tom',\n",
       " 'talent',\n",
       " 'screen',\n",
       " 'surpassed',\n",
       " 'friendship',\n",
       " 'screen',\n",
       " 'thank',\n",
       " 'creating',\n",
       " 'ranscendent',\n",
       " 'cinematic',\n",
       " 'experience',\n",
       " 'thank',\n",
       " 'everybody',\n",
       " 'fox',\n",
       " 'new',\n",
       " 'regency',\n",
       " 'entire',\n",
       " 'team',\n",
       " 'thank',\n",
       " 'everyone',\n",
       " 'onset',\n",
       " 'career',\n",
       " 'parents',\n",
       " 'none',\n",
       " 'would',\n",
       " 'possible',\n",
       " 'without',\n",
       " 'friends',\n",
       " 'love',\n",
       " 'dearly',\n",
       " 'know',\n",
       " 'lastly',\n",
       " 'want',\n",
       " 'say',\n",
       " 'making',\n",
       " 'covenant',\n",
       " 'man',\n",
       " 'relationship',\n",
       " 'natural',\n",
       " 'world',\n",
       " 'world',\n",
       " 'collectively',\n",
       " 'felt',\n",
       " 'hottest',\n",
       " 'year',\n",
       " 'recorded',\n",
       " 'history',\n",
       " 'production',\n",
       " 'needed',\n",
       " 'move',\n",
       " 'southern',\n",
       " 'tip',\n",
       " 'planet',\n",
       " 'able',\n",
       " 'find',\n",
       " 'snow',\n",
       " 'climate',\n",
       " 'change',\n",
       " 'real',\n",
       " 'happening',\n",
       " 'right',\n",
       " 'urgent',\n",
       " 'threat',\n",
       " 'facing',\n",
       " 'entire',\n",
       " 'species',\n",
       " 'need',\n",
       " 'work',\n",
       " 'collectively',\n",
       " 'together',\n",
       " 'stop',\n",
       " 'procrastinating',\n",
       " 'need',\n",
       " 'support',\n",
       " 'leaders',\n",
       " 'around',\n",
       " 'world',\n",
       " 'speak',\n",
       " 'big',\n",
       " 'polluters',\n",
       " 'speak',\n",
       " 'humanity',\n",
       " 'indigenous',\n",
       " 'people',\n",
       " 'world',\n",
       " 'billions',\n",
       " 'billions',\n",
       " 'underprivileged',\n",
       " 'people',\n",
       " 'would',\n",
       " 'affected',\n",
       " 'children',\n",
       " 'children',\n",
       " 'people',\n",
       " 'whose',\n",
       " 'voices',\n",
       " 'drowned',\n",
       " 'politics',\n",
       " 'greed',\n",
       " 'thank',\n",
       " 'amazing',\n",
       " 'award',\n",
       " 'tonight',\n",
       " 'let',\n",
       " 'us',\n",
       " 'take',\n",
       " 'planet',\n",
       " 'granted',\n",
       " 'take',\n",
       " 'tonight',\n",
       " 'granted',\n",
       " 'thank',\n",
       " 'much']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokens_no_typpos = []\n",
    "for word in new_words:\n",
    "    tb = TextBlob(word)\n",
    "    clean_tokens_no_typpos.append(tb.correct().raw)\n",
    "    tb=\"\"\n",
    "clean_tokens_no_typpos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Else we can use Text Blob for word Correction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming v/s Lemmitization\n",
    "* Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language. Example: [finally,final,finalized] = fina\n",
    "* Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma.Example: [finally,final,finalized] = final\n",
    "\n",
    "\n",
    "<b>Making a stemmer is easier and for Lemmitizer we need deep knowledge for fast algorithm go for stemming else Lemmitizer</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thank', 'RB'),\n",
       " ('much', 'JJ'),\n",
       " ('thank', 'NN'),\n",
       " ('academy', 'NN'),\n",
       " ('thank', 'NN'),\n",
       " ('room', 'NN'),\n",
       " ('congratulate', 'NN'),\n",
       " ('incredible', 'JJ'),\n",
       " ('nominee', 'JJ'),\n",
       " ('year', 'NN'),\n",
       " ('covenant', 'NN'),\n",
       " ('product', 'NN'),\n",
       " ('tireless', 'NN'),\n",
       " ('efforts', 'NNS'),\n",
       " ('unbelievable', 'JJ'),\n",
       " ('cast', 'NN'),\n",
       " ('crew', 'NN'),\n",
       " ('first', 'RB'),\n",
       " ('brother', 'RB'),\n",
       " ('endeavor', 'NN'),\n",
       " ('mr', 'NN'),\n",
       " ('tom', 'NN'),\n",
       " ('hardy', 'JJ'),\n",
       " ('tom', 'JJ'),\n",
       " ('talent', 'NN'),\n",
       " ('screen', 'NN'),\n",
       " ('surpassed', 'VBD'),\n",
       " ('friendship', 'NN'),\n",
       " ('screen', 'NN'),\n",
       " ('thank', 'NN'),\n",
       " ('creating', 'VBG'),\n",
       " ('ranscendent', 'JJ'),\n",
       " ('cinematic', 'JJ'),\n",
       " ('experience', 'NN'),\n",
       " ('thank', 'NN'),\n",
       " ('everybody', 'NN'),\n",
       " ('fox', 'VBZ'),\n",
       " ('new', 'JJ'),\n",
       " ('regency', 'NN'),\n",
       " ('entire', 'JJ'),\n",
       " ('team', 'NN'),\n",
       " ('thank', 'NN'),\n",
       " ('everyone', 'NN'),\n",
       " ('onset', 'VBN'),\n",
       " ('career', 'NN'),\n",
       " ('parents', 'NNS'),\n",
       " ('none', 'NN'),\n",
       " ('would', 'MD'),\n",
       " ('possible', 'JJ'),\n",
       " ('without', 'IN'),\n",
       " ('friends', 'NNS'),\n",
       " ('love', 'VBP'),\n",
       " ('dearly', 'RB'),\n",
       " ('know', 'VBP'),\n",
       " ('lastly', 'RB'),\n",
       " ('want', 'VBP'),\n",
       " ('say', 'VBP'),\n",
       " ('making', 'VBG'),\n",
       " ('covenant', 'JJ'),\n",
       " ('man', 'NN'),\n",
       " ('relationship', 'NN'),\n",
       " ('natural', 'JJ'),\n",
       " ('world', 'NN'),\n",
       " ('world', 'NN'),\n",
       " ('collectively', 'RB'),\n",
       " ('felt', 'VBD'),\n",
       " ('hottest', 'JJS'),\n",
       " ('year', 'NN'),\n",
       " ('recorded', 'VBD'),\n",
       " ('history', 'NN'),\n",
       " ('production', 'NN'),\n",
       " ('needed', 'VBN'),\n",
       " ('move', 'NN'),\n",
       " ('southern', 'JJ'),\n",
       " ('tip', 'NN'),\n",
       " ('planet', 'NN'),\n",
       " ('able', 'JJ'),\n",
       " ('find', 'NN'),\n",
       " ('snow', 'JJ'),\n",
       " ('climate', 'NN'),\n",
       " ('change', 'NN'),\n",
       " ('real', 'JJ'),\n",
       " ('happening', 'VBG'),\n",
       " ('right', 'JJ'),\n",
       " ('urgent', 'NN'),\n",
       " ('threat', 'NN'),\n",
       " ('facing', 'VBG'),\n",
       " ('entire', 'JJ'),\n",
       " ('species', 'NNS'),\n",
       " ('need', 'VBP'),\n",
       " ('work', 'NN'),\n",
       " ('collectively', 'RB'),\n",
       " ('together', 'RB'),\n",
       " ('stop', 'VB'),\n",
       " ('procrastinating', 'VBG'),\n",
       " ('need', 'JJ'),\n",
       " ('support', 'NN'),\n",
       " ('leaders', 'NNS'),\n",
       " ('around', 'IN'),\n",
       " ('world', 'NN'),\n",
       " ('speak', 'NN'),\n",
       " ('big', 'JJ'),\n",
       " ('polluters', 'NNS'),\n",
       " ('speak', 'VBP'),\n",
       " ('humanity', 'NN'),\n",
       " ('indigenous', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " ('world', 'NN'),\n",
       " ('billions', 'NNS'),\n",
       " ('billions', 'NNS'),\n",
       " ('underprivileged', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " ('would', 'MD'),\n",
       " ('affected', 'VB'),\n",
       " ('children', 'NNS'),\n",
       " ('children', 'NNS'),\n",
       " ('people', 'NNS'),\n",
       " ('whose', 'WP$'),\n",
       " ('voices', 'NNS'),\n",
       " ('drowned', 'VBD'),\n",
       " ('politics', 'NNS'),\n",
       " ('greed', 'VBD'),\n",
       " ('thank', 'RP'),\n",
       " ('amazing', 'JJ'),\n",
       " ('award', 'NN'),\n",
       " ('tonight', 'NN'),\n",
       " ('let', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('take', 'VB'),\n",
       " ('planet', 'NN'),\n",
       " ('granted', 'VBN'),\n",
       " ('take', 'VBP'),\n",
       " ('tonight', 'NN'),\n",
       " ('granted', 'VBN'),\n",
       " ('thank', 'RB'),\n",
       " ('much', 'JJ')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(clean_tokens_no_typpos)\n",
    "# Now we can get noun or Adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
